version: '2'
services:
  postgres:
    image: postgres:9.6
    volumes:
      - //d/training/local/arf_pg_data:/var/lib/postgresql/data
    ports:
      - "5433"
    environment:
      POSTGRES_USER: fixme
      POSTGRES_PASSWORD: fixme
      POSTGRES_DB: airflow
      PGPORT: 5433
    restart: always
  scheduler:
    depends_on:
      - postgres
    build: &build
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: "2.1"
        SPARK_VERSION: "2.4.7"
        HADOOP_VERSION: "3.1.0"
        SCALA_VERSION: "2.12"
        PYTHON_VERSION: "3.7"
        SQLALCHEMY_VERSION: "1.3"
    command: ["bash", "-c", "python test_db_conn.py && airflow db init && airflow scheduler"]
    volumes:
      - //d/training/local/dags:/airflow/dags
      - //d/training/local/jobs:/airflow/jobs
      - //d/training/local/data:/airflow/data    
      - airflow_logs:/airflow/logs/  
    environment: &environment
      AIRFLOW__CORE__FERNET_KEY: 8NE6O6RcTJpxcCkuKxOHOExzIJkXkeJKbRie03a69dI=
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://fixme:fixme@postgres:5433/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    restart: always
  webserver:
    depends_on:
      - postgres
    build: *build
    command: ["bash", "-c", "sleep 7 && airflow webserver"]
    ports:
      - 8080:8080
    volumes:
      - airflow_logs:/airflow/logs/
    environment: *environment
    restart: always

  pyspark-notebook:
      image: kublr/pyspark-notebook:spark-2.4.0-hadoop-2.6
      volumes:
        - //d/training/local/notebooks:/jupyter
      user: root
      container_name: pyspark-notebook
      hostname: pyspark-notebook
      ports:
        - 8282:8888             
      
volumes:
  # pg_data: 
  #   driver: local
  #   driver_opts:
  #     type: none
  #     o: bind
  #     device: //d/training/local/hive_pg_data
  airflow_logs: {}